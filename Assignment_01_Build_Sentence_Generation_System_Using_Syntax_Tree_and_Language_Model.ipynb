{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson-01 Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 今天是2020年1月05日，今天世界上又多了一名AI工程师 :) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`各位同学大家好，欢迎各位开始学习我们的人工智能课程。这门课程假设大家不具备机器学习和人工智能的知识，但是希望大家具备初级的Python编程能力。根据往期同学的实际反馈，我们课程的完结之后 能力能够超过80%的计算机人工智能/深度学习方向的硕士生的能力。`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 本次作业的内容"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 复现课堂代码\n",
    "\n",
    "在本部分，你需要参照我们给大家的GitHub地址里边的课堂代码，结合课堂内容，复现内容。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 作业截止时间\n",
    "此次作业截止时间为 2020.01.12日"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 完成以下问答和编程练习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基础理论部分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **评阅点**：每道题是否回答完整"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0. Can you come up out 3 sceneraies which use AI methods? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: 1、根据主题不同将新闻自动归类。2、判断一条微博的情绪态度。3、判断一条微博是否是广告。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. How do we use Github; Why do we use Jupyter and Pycharm;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: \n",
    "GitHub 基于 Git 版本控制工具，它一方面可以记录软件从无到有的每个版本，软件是怎么发展过来的，全都一览无遗；另一方面它还可以记录我的编程能力的每一点进步。  \n",
    "GitHub 托管的项目既可以是软件包或者程序代码，也可以是文档教程。因此它也非常适合作为管理学习或教学资料的工具平台。  \n",
    "GitHub是开源的，所有项目的代码和文档，甚至中间过程都是开放的。我可以找一些感兴趣的项目参与其中，可以利用这些项目来提升自己的技术，积累项目经验。  \n",
    "  \n",
    " \n",
    "Jupyter Notebook是一个交互式笔记本，它提供了一个环境，用户可以在里面写代码、运行代码、查看结果，并在其中可视化数据。在进行AI领域的相关工作时，用户可以用Jupyter Notebook便捷地执行各种端到端任务，如数据清洗、统计建模、构建/训练机器学习模型等。  \n",
    "Jupyter Notebook的一个特色是允许把代码写入独立的cell中，然后单独执行。这样做意味着用户可以在测试项目时单独测试特定代码块，无需从头开始执行代码。这非常适合初学者。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. What's the Probability Model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:概率模型是用来描述不同随机变量之间关系的数学模型，通常情况下刻画了一个或多个随机变量之间的相互非确定性的概率关系。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Can you came up with some sceneraies at which we could use Probability Model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:1、连续抛硬币出现正面或反面的次数，我们用“二项分布”概率模型来描述。  \n",
    "  \n",
    "2、地铁每隔5分钟到站，乘客达到这个地铁站的时刻是等可能的，求乘客候车时间不超过3分钟的概率。我们用“几何分布”概率模型来求解（本例是一维连续的均匀分布）。\n",
    "  \n",
    "3、医学中估计白细胞数的正常值范围。我们应用“正态分布”制定一个上限和下限，比如95%的人在正常范围之内，那么超出这一范围的人就认为需要特殊关注。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Why do we use probability and what's the difficult points for programming based on parsing and pattern match?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: \n",
    "摘自《数学之美》：\n",
    "“这里边至少有两个越不过去的坎儿。首先，要想通过文法规则覆盖哪怕20%的真是语句，文法规则的数量（不包括词性标注的规则）至少是几万条。语言学家几乎已经来不及写了，而且这些文法规则写到后来甚至会出现矛盾，为了解决这些矛盾，还要说明各个规则特定的使用环境。如果想要覆盖50%以上的语句，文法规则的数量最后会多到每增加一个新句子，就要加入一些新的文法。...”  \n",
    "“其次，即使能够写出涵盖所有自然语言现象的语法规则集合，用计算机解析也是相当困难的。描述自然语言的文法和计算机高级程序语言的文法不同。自然语言在演变过程中，产生了词义和上下文相关的特性。因此，它的文法是比较复杂的上下文有关文法，而程序语言是我们人为设计的，为了便于计算机解码的上下文无关文法，相比自然语言而言简单得多。理解两者的计算量不可同日而语。”\n",
    "\n",
    "由于上述困难，我们只好放弃基于解析与模式匹配的方法，转向基于概率模型的方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. What's the Language Model;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:  \n",
    "对于语言序列$w_1,w_2,...w_n$，语言模型就是计算该序列的概率，即$Pr(w_1w_2...w_n)$  \n",
    "\n",
    "从机器学习的角度来看，语言模型是对语句的概率分布的建模，作用是判断一个语言序列是否是正常语句，即是否是“人话”。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Can you came up with some sceneraies at which we could use Language Model?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:  \n",
    "1、语音识别，利用语言模型找出最像“人话”的语句。\n",
    "2、在一个语料集中，给定一个词，找出与这个词共现概率最大的其他词。比如有一个粉丝应援团微博的数据集，我们输入明星的名字，得到跟这个明星名字共现最多的词。可以对比不同明星关联词的区别，找出一些有意思的洞察。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. What's the 1-gram language model;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:  \n",
    "N-Gram是一种统计语言模型。它的基本思想是将文本里面的内容按照字节进行大小为N的滑动窗口操作，形成了长度是N的字节片段序列。\n",
    "\n",
    "每一个字节片段称为gram，对所有gram的出现频次进行统计，并且按照事先设定好的阈值（课堂练习没有要求过滤短文本）进行过滤，形成关键gram列表，也就是这个文本的向量特征空间，列表中的每一个gram就是一个特征向量维度。  \n",
    "\n",
    "该模型基于这样一种假设（马尔可夫假设）：第N个词的出现只与前面N-1个词相关，而与其它任何词都不相关。这些概率可以通过直接从语料中统计N个词同时出现的次数得到。  \n",
    "\n",
    "常用的是Bi-Gram和Tri-Gram。  \n",
    "基于上述定义，uni-gram的假设是：每个词的出现只跟自己有关，跟语句中其他词无关。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. What's the disadvantages and advantages of 1-gram language model;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:  \n",
    "uni-gram相对于bi-gram和tri-gram的好处和坏处：  \n",
    "好处：容易理解、容易计算、容易代码实现。  \n",
    "坏处：损失了全部上下文信息，判断语句合理性的准确性相比而言更低。  \n",
    "\n",
    "\n",
    "N-Gram模型的优缺点：  \n",
    "优点：(1) 采用极大似然估计，参数易训练；(2) 完全包含了前 n-1 个词的全部信息；(3) 可解释性强，直观易理解。  \n",
    "\n",
    "缺点：(1) 缺乏长期依赖，只能建模到前 n-1 个词；(2) 随着 n 的增大，参数空间呈指数增长；(3) 数据稀疏，难免会出现OOV(out of vocabulary)的问题；(4) 单纯的基于统计频次，泛化能力差。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. What't the 2-gram models;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:\n",
    "在N-gram模型定义基础上（前面已经回答），bi-gram基于假设：第2个词的出现只与前面1个词相关，而与其它任何词都不相关。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 编程实践部分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 设计你自己的句子生成器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如何生成句子是一个很经典的问题，从1940s开始，图灵提出机器智能的时候，就使用的是人类能不能流畅和计算机进行对话。和计算机对话的一个前提是，计算机能够生成语言。\n",
    "\n",
    "计算机如何能生成语言是一个经典但是又很复杂的问题。 我们课程上为大家介绍的是一种基于规则（Rule Based）的生成方法。该方法虽然提出的时间早，但是现在依然在很多地方能够大显身手。值得说明的是，现在很多很实用的算法，都是很久之前提出的，例如，二分查找提出与1940s, Dijstra算法提出于1960s 等等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在著名的电视剧，电影《西部世界》中，这些机器人们语言生成的方法就是使用的SyntaxTree生成语言的方法。\n",
    "\n",
    "> \n",
    ">\n",
    "\n",
    "![WstWorld](https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1569578233461&di=4adfa7597fb380e7cc0e67190bbd7605&imgtype=0&src=http%3A%2F%2Fs1.sinaimg.cn%2Flarge%2F006eYYfyzy76cmpG3Yb1f)\n",
    "\n",
    "> \n",
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这一部分，需要各位同学首先定义自己的语言。 大家可以先想一个应用场景，然后在这个场景下，定义语法。例如：\n",
    "\n",
    "在西部世界里，一个”人类“的语言可以定义为：\n",
    "``` \n",
    "human = \"\"\"\n",
    "human = 自己 寻找 活动\n",
    "自己 = 我 | 俺 | 我们 \n",
    "寻找 = 看看 | 找找 | 想找点\n",
    "活动 = 乐子 | 玩的\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "一个“接待员”的语言可以定义为\n",
    "```\n",
    "host = \"\"\"\n",
    "host = 寒暄 报数 询问 业务相关 结尾 \n",
    "报数 = 我是 数字 号 ,\n",
    "数字 = 单个数字 | 数字 单个数字 \n",
    "单个数字 = 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 \n",
    "寒暄 = 称谓 打招呼 | 打招呼\n",
    "称谓 = 人称 ,\n",
    "人称 = 先生 | 女士 | 小朋友\n",
    "打招呼 = 你好 | 您好 \n",
    "询问 = 请问你要 | 您需要\n",
    "业务相关 = 玩玩 具体业务\n",
    "玩玩 = 耍一耍 | 玩一玩\n",
    "具体业务 = 喝酒 | 打牌 | 打猎 | 赌博\n",
    "结尾 = 吗？\"\"\"\n",
    "\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请定义你自己的语法: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一个语法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "you_need_replace_this_with_name_you_given = '''\n",
    "# you code here\n",
    "'''\n",
    "# 场景1：老婆支配老公\n",
    "wifeNagging = \"\"\"\n",
    "nag = 称呼 介词 做事\n",
    "称呼 = 老公|孩儿他爸|大坏蛋\n",
    "介词 = 快去|给我\n",
    "做事 = 开瓶酸奶|倒杯水|剥个橘子\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **评阅点**： 是否提出了和课程上区别较大的语法结构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第二个语法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "you_need_replace_this_with_name_you_given = '''\n",
    "# you code here\n",
    "'''\n",
    "\n",
    "# 场景2：老公敷衍老婆 \n",
    "husbandBePerfunctory = \"\"\"\n",
    "perfunctory = 夸赞 称呼 扯理由 建议\n",
    "夸赞 = 赞美词|赞美词 夸赞\n",
    "赞美词 = 美丽的|善良的|善解人意的\n",
    "称呼 = 老婆|老婆大人|媳妇儿|宝贝\n",
    "扯理由 = 我要 事由 ，\n",
    "事由 = 写代码|打游戏|看书|睡觉\n",
    "建议 = 你还是 替代方案 吧。\n",
    "替代方案 = 自己做|找 别人\n",
    "别人 = 爸爸|妈妈\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **评阅点**：是否和上一个语法区别比较大"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: 然后，使用自己之前定义的generate函数，使用此函数生成句子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "善解人意的善良的老婆我要写代码，你还是找爸爸吧。\n"
     ]
    }
   ],
   "source": [
    "# 解析语法树\n",
    "def parseGrammar(grammar_str, split='=', line_split='\\n'):\n",
    "    grammar = {}\n",
    "    for line in grammar_str.split(line_split):\n",
    "        if not line.strip(): continue\n",
    "        exp, stmt = line.split(split)\n",
    "        grammar[exp.strip()] = [s.split() for s in stmt.split('|')]\n",
    "    return grammar\n",
    "\n",
    "# gram = parseGrammar(grammar_str=husbandBePerfunctory)\n",
    "# print(gram)\n",
    "\n",
    "# 基于语法树随机生成语言\n",
    "# 函数依赖：parseGrammar\n",
    "import random\n",
    "def generateLan(grammar, target):\n",
    "    if target not in grammar: return target # means target is a terminal expression\n",
    "    \n",
    "    expaned = [generateLan(grammar, t) for t in random.choice(grammar[target])]\n",
    "    return ''.join([e for e in expaned if e != 'null'])\n",
    "\n",
    "\n",
    "perfunSen = generateLan(grammar=parseGrammar(grammar_str=husbandBePerfunctory), target='perfunctory')\n",
    "print(perfunSen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: 然后，定义一个函数，generate_n，将generate扩展，使其能够生成n个句子:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 善良的善良的老婆大人我要写代码，你还是自己做吧。\n",
      "2 美丽的美丽的善解人意的善良的媳妇儿我要看书，你还是自己做吧。\n",
      "3 善良的宝贝我要睡觉，你还是自己做吧。\n",
      "4 美丽的善解人意的善良的善良的媳妇儿我要看书，你还是自己做吧。\n",
      "5 善良的老婆大人我要写代码，你还是自己做吧。\n",
      "6 善良的善良的老婆我要打游戏，你还是自己做吧。\n",
      "7 善解人意的老婆大人我要看书，你还是找爸爸吧。\n",
      "8 善良的美丽的宝贝我要看书，你还是找妈妈吧。\n",
      "9 美丽的美丽的美丽的善良的美丽的老婆我要看书，你还是找爸爸吧。\n",
      "10 善良的媳妇儿我要睡觉，你还是自己做吧。\n"
     ]
    }
   ],
   "source": [
    "# 基于语法树随机生成n个句子\n",
    "# 函数依赖：parseGrammar, generateLan\n",
    "def generate_nLan(grammar_str, target, n):\n",
    "    grammar = parseGrammar(grammar_str=grammar_str)\n",
    "    \n",
    "    sentence_list=[]\n",
    "    for i in range(n):\n",
    "        sentence_list.append(generateLan(grammar=grammar, target=target))\n",
    "    return sentence_list\n",
    "\n",
    "\n",
    "sentence_list = generate_nLan(grammar_str=husbandBePerfunctory, target='perfunctory', n=10)\n",
    "\n",
    "i=1\n",
    "for sen in sentence_list:\n",
    "    print(str(i)+' '+sen)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **评阅点**; 运行代码，观察是否能够生成多个句子"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 使用新数据源完成语言模型的训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "按照我们上文中定义的`prob_2`函数，我们更换一个文本数据源，获得新的Language Model:\n",
    "\n",
    "1. 下载文本数据集（你可以在以下数据集中任选一个，也可以两个都使用）\n",
    "    + 可选数据集1，保险行业问询对话集： https://github.com/Computing-Intelligence/insuranceqa-corpus-zh/raw/release/corpus/pool/train.txt.gz\n",
    "    + 可选数据集2：豆瓣评论数据集：https://github.com/Computing-Intelligence/datasource/raw/master/movie_comments.csv\n",
    "2. 修改代码，获得新的**2-gram**语言模型\n",
    "    + 进行文本清洗，获得所有的纯文本\n",
    "    + 将这些文本进行切词\n",
    "    + 送入之前定义的语言模型中，判断文本的合理程度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step1：从硬盘文件读取原始数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\programs\\python\\python38\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3050: DtypeWarning: Columns (0,4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>link</th>\n",
       "      <th>name</th>\n",
       "      <th>comment</th>\n",
       "      <th>star</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>吴京意淫到了脑残的地步，看了恶心想吐</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>首映礼看的。太恐怖了这个电影，不讲道理的，完全就是吴京在实现他这个小粉红的英雄梦。各种装备轮...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>吴京的炒作水平不输冯小刚，但小刚至少不会用主旋律来炒作…吴京让人看了不舒服，为了主旋律而主旋...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>凭良心说，好看到不像《战狼1》的续集，完虐《湄公河行动》。</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>中二得很</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id                                        link name  \\\n",
       "0  1  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "1  2  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "2  3  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "3  4  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "4  5  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "\n",
       "                                             comment star  \n",
       "0                                 吴京意淫到了脑残的地步，看了恶心想吐    1  \n",
       "1  首映礼看的。太恐怖了这个电影，不讲道理的，完全就是吴京在实现他这个小粉红的英雄梦。各种装备轮...    2  \n",
       "2  吴京的炒作水平不输冯小刚，但小刚至少不会用主旋律来炒作…吴京让人看了不舒服，为了主旋律而主旋...    2  \n",
       "3                      凭良心说，好看到不像《战狼1》的续集，完虐《湄公河行动》。    4  \n",
       "4                                               中二得很    1  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "filePath = r'D:\\人工智能与自然语言处理006期_开课吧\\第二章第1节-人工智能引论之概率模型与语言自动生成模型\\Assignment-01\\movie_comments.csv'\n",
    "content = pd.read_csv(filePath)\n",
    "content.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "261497\n"
     ]
    }
   ],
   "source": [
    "comments = content['comment'].tolist() #仅保留新闻正文，并转化为列表\n",
    "print(len(comments))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step2:清洗数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "261497\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def token(string):\n",
    "    # we will learn the regular expression next course.\n",
    "    return re.findall('\\w+', string)\n",
    "\n",
    "# ''.join(token(comments[10]))\n",
    "comments_cleaned = [''.join(token(str(c)))for c in comments]\n",
    "print(len(comments_cleaned))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "wfilePath=r'D:\\人工智能与自然语言处理006期_开课吧\\第二章第1节-人工智能引论之概率模型与语言自动生成模型\\Assignment-01\\comments_cleaned.txt'\n",
    "with open(wfilePath,'w',encoding='UTF-8') as wfile:\n",
    "    for com in comments_cleaned:\n",
    "        wfile.write(com+'\\n')\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step3:切词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 41.7 s\n",
      "4490313\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "\n",
    "# 对一行文本做切词\n",
    "def cut(string): return list(jieba.cut(string.strip('\\n')))\n",
    "\n",
    "# 对全部评论做切词\n",
    "# 将切词得到的所有词(token)存到一个list里\n",
    "def cutAllLines(filePath):\n",
    "    token_list = []\n",
    "    with open(filePath,'r',encoding='UTF-8') as file:\n",
    "        for line in file:\n",
    "            token_list+=cut(line)\n",
    "    return token_list\n",
    "\n",
    "\n",
    "%time token_list = cutAllLines(filePath=wfilePath)\n",
    "print(len(token_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step4:统计词频并计算词的3-Gram概率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基于2-Gram的单词出现条件概率的递推过程：  \n",
    "sentence = '其实 就和 随机森林 原理 一样'  \n",
    "Pr(其实&就和&随机森林&原理&一样)  \n",
    "->Pr(其实|就和&随机森林&原理&一样)Pr(就和&随机森林&原理&一样)  \n",
    "->Pr(其实|就和&随机森林&原理&一样)Pr(就和|随机森林&原理&一样)Pr(随机森林|原理&一样)Pr(原理&一样)  \n",
    "\n",
    "做一个简化：每个单词出现的概率只跟它后面2个单词有关。  \n",
    "->Pr(其实|就和&随机森林)Pr(就和|随机森林&原理)Pr(随机森林|原理&一样)Pr(原理&一样)  \n",
    "\n",
    "这其中："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ Pr(其实|就和\\&随机森林) = \\frac{N(\"其实就和随机森林\")}{N(\"就和随机森林\")} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将上述递推过程推广到一般情况： $$Pr(sentence)=Pr(w_1w_2...w_n)= \\prod_i^{n-2} \\frac {N(w_iw_{i+1}w_{i+2})}{N(w_{i+1}w_{i+2})}*Pr(w_{n-1}w_n)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_list = [str(t) for t in token_list] #将token_list中的全部词转为字符串类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['吴京', '意淫', '到', '了', '脑残', '的', '地步', '看', '了', '恶心', '想', '吐', '首映礼', '看', '的', '太', '恐怖', '了', '这个', '电影']\n",
      "恶心\n",
      "真让人\n",
      "男朋友\n"
     ]
    }
   ],
   "source": [
    "print(token_list[:20])\n",
    "print(token_list[-1])\n",
    "print(token_list[-2])\n",
    "print(token_list[-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3]\n",
      "[1, 2]\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "#复习列表索引规则\n",
    "test_list = [1,2,3,4]\n",
    "print(test_list[1:3])\n",
    "print(test_list[:-2])\n",
    "print(test_list[-2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "['吴京', '意淫', '到', '了', '脑残', '的', '地步'] 7  \n",
    "['吴京意淫','意淫到','到了','了脑残','脑残的','的地步'] 6  \n",
    "['吴京意淫到', '意淫到了', '到了脑残', '了脑残的', '脑残的地步'] 5  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.7 s\n"
     ]
    }
   ],
   "source": [
    "# 将TOKEN中相邻2个词组合到一起\n",
    "%time token_2_gram_list = [''.join(token_list[i:i+2]) for i in range(len(token_list)-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['吴京意淫', '意淫到', '到了', '了脑残', '脑残的', '的地步', '地步看', '看了', '了恶心', '恶心想']\n",
      "真让人恶心\n"
     ]
    }
   ],
   "source": [
    "print(token_2_gram_list[:10])\n",
    "print(token_2_gram_list[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.4 s\n"
     ]
    }
   ],
   "source": [
    "# 将TOKEN中相邻3个词组合到一起\n",
    "%time token_3_gram_list = [''.join(token_list[i:i+3]) for i in range(len(token_list)-2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['吴京意淫到', '意淫到了', '到了脑残', '了脑残的', '脑残的地步', '的地步看', '地步看了', '看了恶心', '了恶心想', '恶心想吐']\n",
      "男朋友真让人恶心\n"
     ]
    }
   ],
   "source": [
    "print(token_3_gram_list[:10])\n",
    "print(token_3_gram_list[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.8 s\n"
     ]
    }
   ],
   "source": [
    "# 统计相邻2个词组的词频，生成统计结果列表[(词组, 词频)]\n",
    "%time wordsFreq_2 = Counter(token_2_gram_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordsFreq_2['吴京意淫']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.89 s\n"
     ]
    }
   ],
   "source": [
    "# 统计相邻3个词组的词频，生成统计结果列表[(词组, 词频)]\n",
    "%time wordsFreq_3 = Counter(token_3_gram_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算：相邻3个词组的出现频次 除以 相邻3个词组中后2个出现频次\n",
    "def two_gram_prob(word1, word2, word3):\n",
    "    if word1 + word2 +word3 in wordsFreq_3: return wordsFreq_3[word1+word2+word3] / wordsFreq_2[word2+word3]\n",
    "    # out of vocabulary problem，如果单词没有出现在词库中，设其概率为1/词汇量\n",
    "    else: \n",
    "        return 1 / len(wordsFreq_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.673008901518267e-07"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_gram_prob('吴京', '意淫', '了')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step5：计算一句话的概率（实现3-Gram）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实现上述2-Gram计算公式\n",
    "def get_probablity(sentence):\n",
    "    words_list = cut(sentence) #对一句话做切词\n",
    "#     print(words_list) #打印\n",
    "    sentence_pro = 1 # 初始化这句话的出现概率\n",
    "#     print(words_list[:-2]) #打印\n",
    "    for i, word in enumerate(words_list[:-2]):\n",
    "        next_1 = words_list[i+1]\n",
    "        next_2 = words_list[i+2]\n",
    "        \n",
    "#         print('word'+word)#打印\n",
    "#         print('next_1'+next_1)#打印\n",
    "#         print('next_2'+next_2)#打印\n",
    "        \n",
    "        probability = two_gram_prob(word, next_1, next_2)\n",
    "        \n",
    "#         print('probability:%f'%probability)#打印\n",
    "        \n",
    "        sentence_pro *= probability\n",
    "        \n",
    "#         print(sentence_pro) #打印\n",
    "\n",
    "    sentence_pro *= (wordsFreq_2[words_list[-2]+words_list[-1]]/len(token_2_gram_list)) #乘以公式最后一项：最后2个词的出现概率\n",
    "    return sentence_pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.663077954612815e-29"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_probablity('吴京是一个爱国的演员')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.8376132609997353e-18"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_probablity('吴京意淫到了脑残的地步看了恶心想吐')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **评阅点** 1. 是否使用了新的数据集； 2. csv(txt)数据是否正确解析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 获得最优质的的语言"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当我们能够生成随机的语言并且能判断之后，我们就可以生成更加合理的语言了。请定义 generate_best 函数，该函数输入一个语法 + 语言模型，能够生成**n**个句子，并能选择一个最合理的句子: \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "提示，要实现这个函数，你需要Python的sorted函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 5]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([1, 3, 5, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个函数接受一个参数key，这个参数接受一个函数作为输入，例如"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 4), (2, 5), (4, 4), (5, 0)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([(2, 5), (1, 4), (5, 0), (4, 4)], key=lambda x: x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "能够让list按照第0个元素进行排序."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(5, 0), (1, 4), (4, 4), (2, 5)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([(2, 5), (1, 4), (5, 0), (4, 4)], key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "能够让list按照第1个元素进行排序."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 5), (1, 4), (4, 4), (5, 0)]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([(2, 5), (1, 4), (5, 0), (4, 4)], key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "能够让list按照第1个元素进行排序, 但是是递减的顺序。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "善解人意的媳妇儿我要睡觉，你还是自己做吧。    0.0\n",
      "美丽的媳妇儿我要睡觉，你还是自己做吧。    0.0\n",
      "善解人意的老婆大人我要写代码，你还是自己做吧。    0.0\n",
      "善解人意的善良的老婆大人我要睡觉，你还是找爸爸吧。    0.0\n",
      "善解人意的善良的善良的老婆大人我要看书，你还是找妈妈吧。    0.0\n",
      "善解人意的宝贝我要睡觉，你还是自己做吧。    0.0\n",
      "善解人意的媳妇儿我要打游戏，你还是找爸爸吧。    0.0\n",
      "善解人意的善良的老婆大人我要睡觉，你还是自己做吧。    0.0\n",
      "美丽的善良的老婆我要睡觉，你还是自己做吧。    0.0\n",
      "美丽的老婆我要睡觉，你还是自己做吧。    0.0\n",
      "善解人意的宝贝我要睡觉，你还是自己做吧。    0.0\n",
      "善解人意的媳妇儿我要写代码，你还是自己做吧。    0.0\n",
      "美丽的善解人意的美丽的宝贝我要打游戏，你还是找爸爸吧。    0.0\n",
      "善解人意的善良的老婆我要打游戏，你还是找爸爸吧。    0.0\n",
      "善解人意的宝贝我要睡觉，你还是自己做吧。    0.0\n",
      "美丽的媳妇儿我要打游戏，你还是找妈妈吧。    0.0\n",
      "善良的宝贝我要看书，你还是自己做吧。    0.0\n",
      "美丽的宝贝我要打游戏，你还是找妈妈吧。    0.0\n",
      "美丽的善良的美丽的善良的善解人意的老婆我要看书，你还是自己做吧。    0.0\n",
      "善良的善解人意的善解人意的善良的老婆我要打游戏，你还是自己做吧。    0.0\n",
      "Wall time: 4 ms\n",
      "最优质的一句话：善解人意的媳妇儿我要睡觉，你还是自己做吧。\n"
     ]
    }
   ],
   "source": [
    "# 函数依赖：generate_nLan,get_probablity\n",
    "def generate_bestLan(grammar_str, target, n):\n",
    "    sen_list = generate_nLan(grammar_str, target, n)\n",
    "    sen_pro_list = [] #[(句子,句子的概率)]\n",
    "    for sen in sen_list:\n",
    "        pro = get_probablity(sen)\n",
    "        print(sen+'    '+str(pro)) #打印\n",
    "        sen_pro_list.append((sen,pro))\n",
    "    sen_pro_sorted_list = sorted(sen_pro_list,key=lambda x:x[1],reverse=True)\n",
    "    return sen_pro_sorted_list[0][0]\n",
    "\n",
    "%time best_sen = generate_bestLan(grammar_str=husbandBePerfunctory, target='perfunctory', n=20)\n",
    "print(\"最优质的一句话：%s\"%best_sen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "好了，现在我们实现了自己的第一个AI模型，这个模型能够生成比较接近于人类的语言。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **评阅点**： 是否使用 lambda 语法进行排序"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: 这个模型有什么问题？ 你准备如何提升？ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:生成句子的概率全部是0，这是因为用于训练概率模型的语料是豆瓣影评，而生成的语句跟影评一点关系都没有（是老公敷衍老婆的常用语句）。\n",
    "我的提升思路是：要么重新定义一个影评的Syntax Tree,要么找一个“老公敷衍老婆的常用语料库”。相比较而言，前一种思路更可行。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**评阅点**: 是否提出了比较实际的问题，例如OOV问题，例如数据量，例如变成 3-gram问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 以下内容为可选部分，对于绝大多数同学，能完成以上的项目已经很优秀了，下边的内容如果你还有精力可以试试，但不是必须的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. (Optional) 完成基于Pattern Match的语句问答\n",
    "> 我们的GitHub仓库中，有一个assignment-01-optional-pattern-match，这个难度较大，感兴趣的同学可以挑战一下。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 5. (Optional) 完成阿兰图灵机器智能原始论文的阅读\n",
    "1. 请阅读阿兰图灵关于机器智能的原始论文：https://github.com/Computing-Intelligence/References/blob/master/AI%20%26%20Machine%20Learning/Computer%20Machinery%20and%20Intelligence.pdf \n",
    "2. 并按照GitHub仓库中的论文阅读模板，填写完毕后发送给我: mqgao@kaikeba.com 谢谢"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "各位同学，我们已经完成了自己的第一个AI模型，大家对人工智能可能已经有了一些感觉，人工智能的核心就是，我们如何设计一个模型、程序，在外部的输入变化的时候，我们的程序不变，依然能够解决问题。人工智能是一个很大的领域，目前大家所熟知的深度学习只是其中一小部分，之后也肯定会有更多的方法提出来，但是大家知道人工智能的目标，就知道了之后进步的方向。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后，希望大家对AI不要有恐惧感，这个并不难，大家加油！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1561828422005&di=48d19c16afb6acc9180183a6116088ac&imgtype=0&src=http%3A%2F%2Fb-ssl.duitang.com%2Fuploads%2Fitem%2F201807%2F28%2F20180728150843_BECNF.thumb.224_0.jpeg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
